---
title: "Logistic Regression"
author: "STAT3612"
date: "2021/12/3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
setwd("C:\\Users\\user\\Documents\\HKU\\Y3 s1\\STAT3612 Statistical Machine Learning\\Project")
IncomeData = read.csv("clean.csv")
IncomeData[sapply(IncomeData, is.character)] <- lapply(IncomeData[sapply(IncomeData, is.character)], as.factor)
head(IncomeData)
```

# train test split
```{r}
library(caret)
set.seed(12L)

#training sample = 80%
train_size = floor(0.8 * nrow(IncomeData))
train_ind = sample(seq_len(nrow(IncomeData)), size = train_size)

train_set = IncomeData[train_ind,]
test_set = IncomeData[-train_ind,]

test_y = test_set$income
```


# Logistic Regression with glm
```{r}
start_time <- proc.time()
glm.model <- glm(formula = income~.,
                 data=train_set, 
                 family = binomial(link = "logit"),
                 x = TRUE)
time.logistic <- proc.time() - start_time
time.logistic
```

```{r}
summary(glm.model)
```

Check accuracy on test set; accuracy = 83.21%
```{r}
logit_prob = predict(glm.model, newdata= test_set,type = "response")
logit_pred = as.factor(ifelse(logit_prob > 0.5, "More_than_50K", "Less_or_equal_50K"))
confusionMatrix(logit_pred, test_y)
```

# try adding second-order terms
```{r}
#glm.model2 <- glm(formula = income~.^2,data=train_set, family = binomial(link = "logit"),x = TRUE)
#summary(glm.model2)
```
Test accuracy = 83.52%. The addition of second order terms only increase test accuracy by a insignificant 0.31%. Hence we will not add second order terms.
```{r}
#logit_prob2 = predict(glm.model2, newdata= test_set,type = "response")
#logit_pred2 = as.factor(ifelse(logit_prob2 > 0.5, "More_than_50K", "Less_or_equal_50K"))
#confusionMatrix(logit_pred2, test_y)
```


ANOVA Likelihood ratio test. The function sequentially compares nested models with increasing complexity against the full model by adding one predictor at a time. That is, at each step of the process the algorithm compares a model consisting of one, two, and so on covariates against the full model. Theory can be found here: https://rpubs.com/vassitar/us_census_theory#33_likelihood_ratio_test_statistic
```{r}
anova(glm.model, test="LRT")
```

Observe that fnlwgt is insignificant. Try removing it:
```{r}
glm.model3 <- glm(formula = income~. -fnlwgt,
                 data=train_set, 
                 family = binomial(link = "logit"),
                 x = TRUE)
summary(glm.model3)
```
Test accuracy = 83.24%. Removing fnlwgt increased accuracy on test set. Furthermore, this variable is hard to interpret. Hence, we remove it.
```{r}
logit_prob3 = predict(glm.model3, newdata= test_set,type = "response")
logit_pred3 = as.factor(ifelse(logit_prob3 > 0.5, "More_than_50K", "Less_or_equal_50K"))
confusionMatrix(logit_pred3, test_y)
```
```{r}
IncomeData$fnlwgt = NULL
```


Assumption Checking
```{r}
plot(glm.model3)
```


Check large residuals:

We first observe patterns for the 15 largest residuals
```{r}
r <- residuals(glm.model3)
temp = abs(r)
temp = order(temp,decreasing = TRUE)
IncomeData[temp[1:15],]
```
Observe that white US males that earn >50K and have 0 net.capital.gain, with marital.status = "not-married" or "other", and have education.num <=6 appears frequently.

Using this intuition, we try grouping education.num and age.


```{r}
cutEducation = cut(train_set$education.num, c(-1, 7, 11, 17))
```

```{r}

```